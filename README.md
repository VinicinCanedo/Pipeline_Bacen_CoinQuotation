# ðŸ’± Projeto de Pipeline de Dados - CotaÃ§Ãµes de Moedas (BACEN)

**Objetivo principal:**  
Executar um pipeline ETL que coleta, transforma e disponibiliza dados para anÃ¡lise da **cotaÃ§Ã£o do DÃ³lar (USD) em relaÃ§Ã£o ao Real (BRL)**, apresentando suas cotaÃ§Ãµes diÃ¡rias, taxas de cÃ¢mbio e o status dos boletins diÃ¡rios.

Este projeto implementa um pipeline de dados completo que coleta, transforma, armazena e apresenta visualmente as **cotaÃ§Ãµes diÃ¡rias do DÃ³lar (USD)** por meio da API pÃºblica do Banco Central do Brasil (BACEN).

## ðŸ§  VisÃ£o Geral

A soluÃ§Ã£o automatiza o processo de obtenÃ§Ã£o de cotaÃ§Ãµes cambiais e torna os dados acessÃ­veis de forma visual e interativa via dashboard. O pipeline Ã© orquestrado para rodar diariamente de forma automÃ¡tica, buscando apenas os dados mais recentes para otimizar o processo.

---

## ðŸ§° Tecnologias Utilizadas

| Etapa | Ferramenta | DescriÃ§Ã£o |
| --- | --- | --- |
| Dados Externos | API BACEN | Fonte de dados de cÃ¢mbio em tempo real |
| Linguagem | `Python` | Linguagem principal para o desenvolvimento do pipeline e dashboard |
| DependÃªncias | `Poetry` | Gerenciamento de dependÃªncias do pipeline ETL |
| Extract | `requests` | Coleta os dados da API REST do BACEN |
| Transform | `Python` | NormalizaÃ§Ã£o e tratamento dos dados coletados |
| Load | `Supabase` | Banco de dados relacional (PostgreSQL) na nuvem |
| Dashboard | `Streamlit` | VisualizaÃ§Ã£o e anÃ¡lise interativa em tempo real |
| OrquestraÃ§Ã£o | `GitHub Actions` | AutomaÃ§Ã£o e agendamento da execuÃ§Ã£o diÃ¡ria do pipeline |
| ContainerizaÃ§Ã£o | `Docker` | Empacotamento e execuÃ§Ã£o isolada do pipeline e do dashboard |

---

## âš™ï¸ ConfiguraÃ§Ã£o do Ambiente

Antes de executar o projeto, siga estes passos para configurar o banco de dados e as credenciais.

### 1. Configurando o Supabase (Banco de Dados)

VocÃª precisarÃ¡ de uma conta gratuita no [Supabase](https://supabase.com/).

**Passo 1: Crie um novo projeto**
- ApÃ³s fazer login, clique em "New project".
- Escolha um nome para o projeto e gere uma senha segura para o banco de dados. Guarde essa senha.
- Selecione a regiÃ£o mais prÃ³xima de vocÃª e clique em "Create new project".

**Passo 2: Crie a tabela de cotaÃ§Ãµes**
- No menu lateral esquerdo, vÃ¡ para **SQL Editor**.
- Clique em **New query**.
- Copie e cole o script SQL abaixo e clique em **RUN**. Isso criarÃ¡ a tabela `DollarQuotation` com a estrutura correta.


```sql
CREATE TABLE "DollarQuotation" (
    "paridadeCompra" float8,
    "paridadeVenda" float8,
    "cotacaoCompra" float8,
    "cotacaoVenda" float8,
    "dataHoraCotacao" timestamp with time zone NOT NULL,
    "tipoBoletim" text,
    CONSTRAINT "DollarQuotation_pkey" PRIMARY KEY ("dataHoraCotacao")
);
```

**Passo 3: Obtenha a URL e a Chave de API**
- No menu lateral esquerdo, vÃ¡ para **Project Settings** (Ã­cone de engrenagem).
- Selecione **API**.
- VocÃª encontrarÃ¡ as informaÃ§Ãµes que precisa:
    - Em **Project URL**, copie a URL.
    - Em **Project API Keys**, copie a chave `service_role`. **AtenÃ§Ã£o:** esta chave tem privilÃ©gios de administrador. Mantenha-a segura e nÃ£o a exponha publicamente.

### 2. Configurando o Pipeline (Arquivo `.env`)

O pipeline ETL usa um arquivo `.env` para se conectar ao Supabase.

- Na raiz do seu projeto, crie um arquivo chamado `.env`.
- Adicione a URL e a chave que vocÃª copiou no passo anterior:

```properties
# filepath: .env
SUPABASE_URL="SUA_URL_DO_SUPABASE_AQUI"
SUPABASE_KEY="SUA_CHAVE_SERVICE_ROLE_AQUI"
```

### 3. Configurando o Dashboard (`secrets.toml`)

O dashboard Streamlit usa um arquivo de segredos para se conectar de forma segura.

- Na raiz do seu projeto, crie uma pasta chamada `.streamlit`.
- Dentro da pasta `.streamlit`, crie um arquivo chamado `secrets.toml`.
- Adicione as mesmas credenciais do Supabase a este arquivo:

```toml
# filepath: .streamlit/secrets.toml
SUPABASE_URL="SUA_URL_DO_SUPABASE_AQUI"
SUPABASE_KEY="SUA_CHAVE_SERVICE_ROLE_AQUI"
```
**Importante:** Certifique-se de que este arquivo estÃ¡ salvo com a codificaÃ§Ã£o **UTF-8**. No VS Code, vocÃª pode verificar e alterar a codificaÃ§Ã£o na barra de status inferior direita.

---

## ðŸ³ Como Executar com Docker

Com o ambiente configurado, vocÃª pode construir e executar as aplicaÃ§Ãµes com os seguintes comandos.

**1. Construa a imagem do Pipeline ETL:**
Este comando usa o `Dockerfile` principal para criar a imagem que executarÃ¡ o script de extraÃ§Ã£o.

```bash
docker build -f Dockerfile -t pipeline-bacen .
```

**2. Execute o Pipeline ETL:**
Este comando executa o container, injetando as credenciais do seu arquivo `.env`. O pipeline buscarÃ¡ os dados e os salvarÃ¡ no seu banco Supabase.

```bash
docker run --env-file .env pipeline-bacen
```

**3. Construa a imagem do Dashboard:**
Este comando usa o `Dockerfile.dashboard` para criar a imagem que servirÃ¡ a aplicaÃ§Ã£o Streamlit.

```bash
docker build -f Dockerfile.dashboard -t dashboard-bacen .
```

**4. Inicie o Dashboard Streamlit:**
Este comando executa o container do dashboard e mapeia a porta `8501` para que vocÃª possa acessÃ¡-lo no seu navegador.

```bash
docker run -p 8501:8501 dashboard-bacen
```

- ApÃ³s executar, acesse **`http://localhost:8501`** no seu navegador para ver o dashboard.

---

## ðŸ” Etapas da Pipeline

1.  **ExtraÃ§Ã£o de Dados**
    O pipeline consulta o Supabase para obter a data do Ãºltimo registro. Em seguida, busca na API do BACEN apenas os dados a partir dessa data atÃ© o dia atual, otimizando a coleta.

2.  **TransformaÃ§Ã£o de Dados**
    Os dados brutos em JSON sÃ£o processados com Python: os tipos de dados sÃ£o corrigidos, e duplicatas na mesma carga sÃ£o removidas para garantir a integridade.

3.  **Carga em Supabase**
    Antes de inserir, o pipeline verifica novamente quais registros jÃ¡ existem no banco para evitar erros de duplicidade. Apenas os dados 100% novos sÃ£o carregados na tabela `DollarQuotation`.

4.  **VisualizaÃ§Ã£o via Streamlit**
    O dashboard se conecta diretamente ao Supabase, garantindo que os dados exibidos estejam sempre atualizados. Ele oferece grÃ¡ficos de linha, barras e mÃ©dias mÃ³veis, com filtros interativos.

---

## ðŸš€ ExecuÃ§Ã£o AutomÃ¡tica com GitHub Actions

VocÃª pode agendar a execuÃ§Ã£o diÃ¡ria do pipeline ETL de forma totalmente automÃ¡tica usando o GitHub Actions. Siga o passo a passo abaixo:

### 1. Crie o workflow do GitHub Actions

- No seu repositÃ³rio, crie a pasta `.github/workflows` (caso ainda nÃ£o exista).
- Dentro dela, crie um arquivo chamado `pipeline-etl.yml` com o seguinte conteÃºdo:

```yaml
name: Pipeline ETL Bacen

on:
  schedule:
    - cron: '0 8 * * *'  # Executa todos os dias Ã s 8h UTC
  workflow_dispatch:      # Permite execuÃ§Ã£o manual pelo GitHub

jobs:
  run-pipeline:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout do cÃ³digo
        uses: actions/checkout@v4

      - name: Configurar Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Instalar dependÃªncias
        run: |
          pip install poetry
          poetry install

      - name: Configurar variÃ¡veis de ambiente
        run: |
          echo "SUPABASE_URL=${{ secrets.SUPABASE_URL }}" >> $GITHUB_ENV
          echo "SUPABASE_KEY=${{ secrets.SUPABASE_KEY }}" >> $GITHUB_ENV

      - name: Executar pipeline ETL
        run: |
          poetry run python Pipeline/main.py
```

### 2. Configure os segredos do repositÃ³rio

- No GitHub, acesse seu repositÃ³rio > **Settings** > **Secrets and variables** > **Actions** > **New repository secret**.
- Adicione os segredos:
  - `SUPABASE_URL` (URL do seu projeto Supabase)
  - `SUPABASE_KEY` (chave service_role do Supabase)

### 3. Pronto!

- O pipeline serÃ¡ executado automaticamente todos os dias no horÃ¡rio agendado.
- VocÃª tambÃ©m pode rodar manualmente em **Actions** > **Pipeline ETL Bacen** > **Run workflow**.

---